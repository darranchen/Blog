---
layout: post
title: "统计复习资料总结"
date: 2014-09-17 23:34:00
categories: Statistics
tags: 统计
keywords: 统计 
---

看了一下一些招数据分析或者数据挖掘的笔试题目，默默觉得这广度也太大了，所以要重新整理一下学过的统计理论，系统的复习一遍。先从最基本的统计概念开始：

统计从两个分类开始 1. 描述性统计量， 2. 推论统计

# 描述性统计

描述性统计量，从名字就可以知道描述性统计量是描述性的，也就是对于我们关心的东西最基本的了解，比如拿到一个人的资料，那么对于这个人的资料做一些总结，概括，就是描述性统计量在做的事情，什么是统计量的，简单而言就是得到的资料(样本)的函数，由样本组成转化而来的。 拿到一笔资料之后关心的统计量到底有哪些呢？ 

## 单个个体

先从单个个体关心的内容讲起， 比如关心的是一群学生的成绩，或者说是汽水的品牌市场占有量，这些都是单个个体， 上面其实就说明了基本上统计关心的数据类型有 

1. 定类数据(Qualitative factor)
2. 定序数据(Order qualitative factor)
3. 定距数据
4. 定比数据(Quantitive factor)

一般我们常接触到的是定类的数据还有定比的数据，其实台湾常用的就是类别型的资料和连续型的资料，对于不同的资料类型，我们对于单个个体就想知道他的大致分布情况，看大致分布情况最简单的就是找一个代表性的量来代表这个群体， 除了代表性的量，还有看他的散步状态，也就是需要考虑它的变异情况，不确定情况了。首先找一个代表性的量有哪些呢？

1. 中位数
2. 众数
3. 平均数

对于不同种数据类型，可以应用的数不一样，类别型一般代表性就是看众数，也就是看哪个类别出现最多。 那么类别型变数使用众数是不是一个合理代表的量， 那么就要看异众比率了， 异众比率是指总体中非众数次数和总次数的比

$$ V_i = \frac{\sum_i f_i - f_m}{\sum_i f_i} $$

这个比率越大，众数代表性越差，比率越小，代表性越好。



中位数和平均数都有大小的观念，所以就只适用于连续性的资料。

这几个量没有固定的大小关系，那么什么时候适合众数，什么时候用中位数，什么时候用平均数的，用平均数是因为平均数有很好的数学性质，便于做推论，要做代表性的指标的话其实当资料是对称的时候比较有代表，但是当资料发生严重左偏或者右偏的时候就比较适合使用中位数或者众数了。

只是一个数字是不够代表的，比如当两个人平均分一样的时候，我们就看稳定度或者是分散情况了，稳定度的话怎么解读，有几个量可以作为解读的

1. 四分位差 就是前75%的位数减去前25%的位数
2. 极距
3. 方差标准差(变异数)
4. 偏度 
5. 峰值

这里就会用到中心距跟原心距的问题了。

k阶距原心距的计算公式是

$$ \sum x_i^k $$

中心距的公式是 

$$ \sum (x_i-\bar{x})^k$$

方差在这里就是二阶距，而偏度就是三阶距

$$ \alpha_3 = \frac{\sum_{i=1}^n (x_i-\bar{x})^3}{ns_n^3}$$

如果 $\alpha_3 >0 $ 正偏态（右偏)，如果 $\alpha_3<0 $ 负偏态(左偏).

而峰度是看中间集中的高度， 


$$ \alpha_4 = \frac{\sum_{i=1}^n (x_i-\bar{x})^4}{ns_n^4}$$

如果 $\alpha_4 >3 $ 尖顶峰，如果 $\alpha_4<3 $ 平顶峰 , $\alpha_4 = 3$ 正态峰。

## 多个个体

解决完单个个体，下面就需要考虑有多个个体，先考虑只有两个个体，也就是你看到了学生有 x, y这两门考试的成绩，那么就要去描述他们之间的关系了。如果是连续型的，关系最简单就是皮尔森相关系数r

$$ r = \frac{1}{n-1}\sum_{i=1}^n (\frac{x_i -\bar{x}}{s_x})(\frac{y_i -\bar{y}}{s_y}) $$


这个只是统计上面的比较关联性，那么如果去比较他们的差距，就是差距情况有很多数学上面的distance，用于衡量距离的量需要满足四个条件

1. 当且仅当 x = y 时 d(x,y) = 0.
2. d(x,y) >0 
3. d(x,y) = d(y,x)
4. $d(x,y) \leq d(x,z)+d(y,z)$ 三角不等式。

* 欧几里德距离

$$ dist(x,y) = \sqrt{\sum （x_i-y_i)^2}$$

简单暴力，需要标准化，受到绝对大小的影响

* 马氏距离 (Mahalanobis distance)

这个其实是把两个向量 x 和y 相关性考虑进来了。前提是先假设 x 和 y是服从同一个多变量分布且协方差矩阵( correlation matrix) 为 $\Sigma$, 那么他们之间的距离就为

$$ dist(x,y) = x'\Sigma y $$

不受样本大小的影响，缺点就是对于x 这个代表不同属性的每个元素 $x_1, x_2$ 当作同样权重的比较。

* 闵可夫斯基距离(Minkowsk distance)

这个算是欧式距离的推广，一般p =3 

$$ dist(x,y) = (\sum\|x_i-y_i\|^p)^{\frac{1}{p}} $$


当p = 1 时，就变成了 曼哈顿距离 Manhattan Distance

$$ dist(x,y) = \sum (\|x_i-y_i\|) $$

当 p = $\infty$ 时就是切比雪夫距离

$$ dist(x,y) = max \|x_i-y_i\| $$

这里的概念其实就是范数的概念。 0 范数就是计算向量中非0的元素的个数.


* 汉明距离(Hamming distance)

信息论里面的概念， 两个等长字符串之间相比对应位置不同字符的个数。

比如010 与110 他们的距离就是1. 
toned 和rouse 的距离是3

* Jaccard 系数

这个其实应用于文本距离，比如一个商品描述是 iphone, 全新， 另一个商品是 iphone, 二手， 那么距离就是把这些描述当作是一个集合，

$$ Jaccard(x,y) = \frac{\|x \cap y\|}{\| x \cup y\|}$$

这个距离来描述他们相对距离的大小，如果是1，那么相关性最大。

* 余弦相似度 

$$ sim(x,y) = \cos\theta = \frac{\vec{x}\vec{y}}{\|x\|\|y\|} = \frac{\sum x_iy_i}{\sqrt{\sum x_i^2}\sqrt{\sum y_i^2}}$$

这个其实是跟皮尔森相关系数很近似的一个值，如果都减去他们的平均数之后，那么就一样了

* Tonimoto 系数( 广义的 Jaccard 系数)

$$ d(x,y) =  \frac{\sum x_iy_i}{\sqrt{\sum x_i^2}+\sqrt{\sum y_i^2}-sum x_iy_i}$$

除了皮尔森相关系数之外，还有一个spearman 相关系数，之前的皮尔森相关系数似乎有要求要服从正态分布，之前做的资料没有遇到这个状况产生的问题，可能是由于接触资料是连续的，一般都认为连续近似常态了。说一下 spearman 的相关系数好了。这个其实是将原始资料 x 和y 重新排序了一下， 然后把x,y 转化为他们的大小顺序 X，Y，有了大小顺序就可以比较他们的相关程度了。这里需要注意的是，x,y中有可能会有相同的值，那么他们的大小顺序一样了，所以需要分情况计算， 

* 无大小相同的值

$$ r_s = 1- \frac{6\sum d_i^2}{n(n^2-1)} $$


* 有大小相同的值

$$ r_s = \frac{\sum (X_i - \bar{X})(Y_i - \bar{Y})}{\sqrt{\sum(X_i-\bar{X})^2}}$$

还有很多奇奇怪怪的相关系数，暂时不做过多了解了。

# 统计推论

统计推论的目的在于我们看到了上面的统计量之后，就问了一句，然后呢。有了这些平均数，或者说方差之后，这些都是样本，那么我们关心的母体的情况怎么样。 基本想法是加入的分布假设。有了这些假设就可以做关心的内容了。

第一个就是看了这些样本之后，那么就会猜我们母体的情况，这里就会加入分配假设的概念，比如我们假设这些样本 x是服从某个分配， 这些分配可以有很多，可以有

* 二项分布(伯努利分布) 也就是从抽n次，成功的机率为p。

关心的量是 p 也就是成功的机率， 那么p值的变异数是 $\frac{p(1-p)}{n}$

* 正态分布 $N(\mu, \sigma^2)$.

pdf 的函数是 

$$ f(x) = \frac{1}{\sqrt{2\pi}\sigma}\exp{(-\frac{(x-\mu)^2}{2\sigma^2})} $$

关心的量就是就是 $\mu, \sigma$

* 卡方分布

如果是 x服从正态分布， 那么 x^2就是服从卡方分布(1)。需要确定的量就是资料读

* t 分布

如果x 服从正态分布， y是服从 卡方(n) 的，那么 

$$ z = \frac{x}{y} $$

就是自由度为n的t 分布

* F 分布

如果x 服从卡方(m)， y是服从 卡方(n) 的，那么 

$$ z = \frac{x}{y} $$

就是自由度为(m,n)的F分布


* 泊松分布

这个是描述在一个点，在极短时间内来的人数的分布，它是离散的，机率分布是

$$ p(x = 0,1,2,\ldots) = \frac{\lambda^x e^{-\lambda}}{x!}$$

泊松分布关心的量是 $\lambda$, 这个量其实可以从二项分布的角度来看， $\lambda = np$ 也就是lambda 是期望发生的个数， 泊松分布的期望值也是 $\lambda$

* 指数分布

指数分布其实跟泊松分布对应的， 也就是在这个时间点等，第一个人来的等待时间，等待的机率分布是

$$ f(x) = \lambda e^{-\lambda x} (x>0)$$

它的期望值刚好是 $\frac{1}{\lambda}$ 也就是在这一段时间内只来了一个人。

* gamma 分布

如果 $x_1, x_2, \ldots, x_k$ 是服从指数型分布，那么 $\sum x_i $就是服从Gamma (k,\theta)的分布,机率分布是

$$ f(x;k,\theta) = \frac{x^{k-1}e^{-\frac{x}{\theta}}}{\theta^k}\Gamma(k) (x>0)$$

这里的k就是等多少个事件发生， $\theta$ 就是 $\lambda$ 的倒数。

大致的机率分布就是这样，我们就是假设了观察的母体其实是服从这样的分布的，所以我们要做的是估计这些参数的值， 比如我们需要估计的是二项分布的参数 p，估计的方式一般而言有两种方式

1. 最大似然估计
2. 最小平方法估计

不同种估计的方法会有不同的估计值，所以估计值方面关心的有两样， 

1. 不偏，也就是期望值等于假设分配的真实值。
2. 变异数最小， 也就是变异数越小的话，越准确。

这是点估计，但是单纯一个点估计真的可以碰到真实值的机率很小，只能说很接近，所以需要区间估计。 区间估计使用的是置信区间(台湾是信赖区间), 95\%置信区间其实就是按照原来随机抽100次，得到100个这样的区间，这100个里面会有95次包括了真实的区间。 注意一点就是，置信区间只是在把由于随机抽样得到的误差描述出来，其他的误差并没有包括在内。

## 检定

上面只是针对母体参数做一些估计， 之后需要做一些检定母体的值是不是等于某一个， 比如我们要看一个班级的平均分是否等于60分这件事情，其实就是对于样本得到的平均分做一个95\%的置信区间，如果在里面就接受假设，如果不在里面就拒绝假设。常见的检定有

* 二项式检定
* z 检定
* t 检定
* 卡方检定
* F 鉴定

这些检定的名称都是根据关心的统计量所服从的分配来确定的。这里主要要讲一下卡方检定， 卡方检定有多个功能

1. 似然检定， 这个是用于检定加入了一个变量之后，它的似然机率值变化有没有足够大，似然函数的比值是 $ D= -2 ln(likelihood for null model)+2ln(likelihood for alternative model)$ 是服从卡方分布

2. Goodness of fit test 检验是否满足期望的分布的检定 $ K^2 = \sum \frac{(O_i-E_i)^2)}{E_i}$ 是服从卡方分布的。

3. 独立性检定，就是看两个类别变量是否独立 $K^2 = \frac{n(ad-bc)^2}{(a+b)(c+d)(a+c)(b+d)}也是服从卡方分布(a+b+c+d)




[Rblog]: http://www.r-bloggers.com "R-bloggers"
[sim]: http://www.r-bloggers.com/simpsons-paradox-is-back/
